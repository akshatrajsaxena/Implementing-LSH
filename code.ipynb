{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSH Implementation \n",
    "\n",
    "The Implementation of the project is divided into four substeps which includes::\n",
    "\n",
    "## 1. **Reading the Input Content and Text Preprocessing**\n",
    "\n",
    "This involves loading data from the following files:\n",
    "- `ids.txt`: Contains the document IDs.\n",
    "- `texts.txt`: Contains the text data corresponding to the document IDs.\n",
    "\n",
    "Text preprocessing typically involves several operations:\n",
    "- **Lowercase conversion**: Ensures uniformity by converting all text to lowercase.\n",
    "- **Removing punctuation**: Cleans the text to focus on words.\n",
    "- **Tokenization**: Splits the text into individual words or tokens.\n",
    "\n",
    "Further, the preprocessing may also include:\n",
    "- **Stopword removal**: Eliminates common words (e.g., \"the\", \"is\", \"and\") which do not contribute much meaning.\n",
    "- **Stemming/Lemmatization**: Reduces words to their base forms (e.g., \"running\" â†’ \"run\") to standardize the text, improving the comparison process.\n",
    "\n",
    "## 2. **Vectorizing the Text with TF-IDF**\n",
    "\n",
    "**TF-IDF** (Term Frequency-Inverse Document Frequency) is a technique used to convert text into numerical vectors. The process weighs terms based on two factors:\n",
    "\n",
    "- **Term Frequency (TF)**: Measures how often a word appears in a document.\n",
    "- **Inverse Document Frequency (IDF)**: Measures how rare a word is across all documents.\n",
    "\n",
    "This method captures the relative importance of words, allowing for meaningful comparisons between documents. TF-IDF representation is effective for identifying key terms and measuring document similarity.\n",
    "\n",
    "## 2. **Vectorizing the Text with TF-IDF**\n",
    "\n",
    "**TF-IDF** (Term Frequency-Inverse Document Frequency) is a technique used to convert text into numerical vectors. The process weighs terms based on two factors:\n",
    "\n",
    "- **Term Frequency (TF)**: Measures how often a word appears in a document.\n",
    "- **Inverse Document Frequency (IDF)**: Measures how rare a word is across all documents.\n",
    "\n",
    "This method captures the relative importance of words, allowing for meaningful comparisons between documents. TF-IDF representation is effective for identifying key terms and measuring document similarity.\n",
    "\n",
    "---\n",
    "\n",
    "### Proper Working of TF-IDF:\n",
    "\n",
    "**TF-IDF** stands for Term Frequency-Inverse Document Frequency. It measures how important a word is in a document within a collection of documents.\n",
    "\n",
    "The TF-IDF value for a term is calculated by multiplying two metrics:\n",
    "\n",
    "1. **Term Frequency (TF)**: The number of times a term appears in a document, normalized by the total number of terms in the document.\n",
    "\n",
    "   $$\n",
    "   TF(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}\n",
    "   $$\n",
    "\n",
    "2. **Inverse Document Frequency (IDF)**: A measure of how rare a term is across all documents. The more rare a term is, the higher its IDF value.\n",
    "\n",
    "   $$\n",
    "   IDF(t) = \\log \\left(\\frac{N}{n_t}\\right)\n",
    "   $$\n",
    "\n",
    "   Where:\n",
    "   - \\( N \\) = Total number of documents\n",
    "   - \\( n<sub>t</sub> \\) = Number of documents containing the term \\( t \\)\n",
    "\n",
    "The final **TF-IDF** score for a term is the product of the term frequency and inverse document frequency:\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d) = TF(t, d) \\times IDF(t)\n",
    "$$\n",
    "---\n",
    "\n",
    "By applying this formula, TF-IDF identifies important words in the document that are also relatively unique across the corpus.\n",
    "\n",
    "\n",
    "## 3. **Implementing LSH for Finding Similarity**\n",
    "\n",
    "**LSH** (Locality-Sensitive Hashing) is used to approximate similarity search, enabling efficient identification of similar documents. It works by:\n",
    "- Hashing similar items into the same \"buckets\" with high probability.\n",
    "\n",
    "For text data, **MinHash** is commonly employed. It creates multiple hash functions to generate a signature for each document, allowing quick retrieval of potential similar items without performing exhaustive pairwise comparisons.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Evaluating the Accuracy by Comparing with Ground Truth**\n",
    "\n",
    "This step evaluates the performance of the LSH implementation by comparing its results to the provided ground truth in the `items.json` file. The evaluation process involves:\n",
    "- Calculating **intersection scores** between the predicted and actual similar items for each document.\n",
    "- Computing the **average score** across all documents to assess the overall model performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (3.8.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.14.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (3.8.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.14.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (3.8.4)\n",
      "Requirement already satisfied: seaborn in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (0.13.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (2.2.2)\n",
      "Requirement already satisfied: datasketch in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (1.6.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from datasketch) (1.14.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\raksh\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# Installing the required libraries which might not be available in the environment.\n",
    "!pip install scikit-learn numpy pandas matplotlib\n",
    "!pip install numpy pandas matplotlib scikit-learn\n",
    "!pip install numpy matplotlib seaborn pandas datasketch\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(words, n=2):\n",
    "    return [' '.join(gram) for gram in ngrams(words, n)]\n",
    "\n",
    "\n",
    "def create_shingles(text, n=2):\n",
    "    words = preprocess_text(text)\n",
    "    unigrams = words\n",
    "    bigrams = create_ngrams(words, 2)\n",
    "    trigrams = create_ngrams(words, 3)\n",
    "    return set(unigrams + bigrams + trigrams)\n",
    "\n",
    "\n",
    "def create_minhash(shingles, num_perm=256):\n",
    "    minhash = MinHash(num_perm=num_perm)\n",
    "    for shingle in shingles:\n",
    "        minhash.update(shingle.encode('utf-8'))\n",
    "    return minhash\n",
    "\n",
    "\n",
    "def build_lsh_index(ids, texts, threshold=0.5, num_perm=256):\n",
    "    lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
    "    for id, text in zip(ids, texts):\n",
    "        shingles = create_shingles(text)\n",
    "        lsh.insert(id, create_minhash(shingles, num_perm))\n",
    "    return lsh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "\n",
    "def find_similar_items(lsh, ids, texts, num_results=5):\n",
    "    similar_items = {}\n",
    "    for id, text in zip(ids, texts):\n",
    "        query_shingles = create_shingles(text)\n",
    "        query_minhash = create_minhash(query_shingles)\n",
    "        results = lsh.query(query_minhash)\n",
    "        results = [r for r in results if r != id]\n",
    "\n",
    "        similarities = []\n",
    "        for result_id in results:\n",
    "            result_shingles = create_shingles(texts[ids.index(result_id)])\n",
    "            similarity = jaccard_similarity(query_shingles, result_shingles)\n",
    "            similarities.append((result_id, similarity))\n",
    "\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        similar_items[id] = [item[0] for item in similarities[:num_results]]\n",
    "\n",
    "    return similar_items\n",
    "\n",
    "\n",
    "def evaluate_model(similar_items, ground_truth):\n",
    "    intersection_scores = []\n",
    "    for id, predicted in similar_items.items():\n",
    "        truth = ground_truth.get(id, [])\n",
    "        intersection = set(predicted) & set(truth)\n",
    "        intersection_scores.append(len(intersection))\n",
    "    return intersection_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input_files(ids_file, texts_file, items_file):\n",
    "    with open(ids_file, 'r', encoding='utf-8') as f:\n",
    "        ids = [line.strip() for line in f]\n",
    "    with open(texts_file, 'r', encoding='utf-8') as f:\n",
    "        texts = [line.strip() for line in f]\n",
    "    with open(items_file, 'r', encoding='utf-8') as f:\n",
    "        ground_truth = json.load(f)\n",
    "    return ids, texts, ground_truth\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(intersection_scores):\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(intersection_scores, bins=6, range=(0, 5), edgecolor='black')\n",
    "    plt.title('Histogram of Intersection Scores')\n",
    "    plt.xlabel('Intersection Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.savefig('histogram.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.boxplot(x=intersection_scores)\n",
    "    plt.title('Box Plot of Intersection Scores')\n",
    "    plt.xlabel('Intersection Score')\n",
    "    plt.savefig('boxplot.png')\n",
    "    plt.close()\n",
    "\n",
    "    stats = pd.Series(intersection_scores).describe()\n",
    "    print(stats)\n",
    "    stats.to_csv('descriptive_stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    148928.000000\n",
      "mean          0.001081\n",
      "std           0.033065\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           0.000000\n",
      "max           2.000000\n",
      "dtype: float64\n",
      "Average Intersection Score: 0.001081059303824667\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    ids, texts, ground_truth = read_input_files(\n",
    "        'ids.txt', 'texts.txt', 'items.json')\n",
    "    lsh = build_lsh_index(ids, texts)\n",
    "    similar_items = find_similar_items(lsh, ids, texts)\n",
    "\n",
    "    with open('lsh_similar_items.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(similar_items, f, indent=2)\n",
    "    intersection_scores = evaluate_model(similar_items, ground_truth)\n",
    "    visualize_results(intersection_scores)\n",
    "\n",
    "    average_score = sum(intersection_scores) / len(intersection_scores)\n",
    "    print(f\"Average Intersection Score: {average_score}\")\n",
    "    with open('average_score.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"Average Intersection Score: {average_score}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
